{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37ebc58",
   "metadata": {},
   "source": [
    "The Core Math: Input * Weight + Bias\n",
    "\n",
    "Inputs -> Enemy position, player position, enemy health, player health\n",
    "\n",
    "Weights -> How much important is current input is.\n",
    "\n",
    "Bias -> Activation threshold.\n",
    "\n",
    "Output = Sigmoid(Input * Weight + Bias)\n",
    "\n",
    "return Output > 0.5 {true -> the neurone is fire}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e89da",
   "metadata": {},
   "source": [
    "Q1. Teach AI to learn to Double the number give it inputs [1, 2, 3, 4] and expect [2, 4, 6, 8]. \n",
    "Initially, it will guess wrong, but it will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9adcf360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start weight: 0.632358, Start Bais: -0.713767\n",
      "\n",
      "Training...\n",
      "Epoch 0 loss: 19.418684\n",
      "Epoch 100 loss: 0.004552\n",
      "Epoch 200 loss: 0.002499\n",
      "Epoch 300 loss: 0.001372\n",
      "Epoch 400 loss: 0.000753\n",
      "Epoch 500 loss: 0.000414\n",
      "Epoch 600 loss: 0.000227\n",
      "Epoch 700 loss: 0.000125\n",
      "Epoch 800 loss: 0.000068\n",
      "Epoch 900 loss: 0.000038\n",
      "\n",
      "--- RESULTS ---\n",
      "Final Weight (should be ~2.0): 2.0038\n",
      "Final Bias (should be ~0.0): -0.0111\n",
      "Prediction for input 10.0: 20.0267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data(Teacher):\n",
    "inputs = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "targets = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Model(Brain)\n",
    "model = nn.Linear(1, 1) # 1 input and 1 output\n",
    "\n",
    "print(f\"start weight: {model.weight.item():2f}, Start Bais: {model.bias.item():2f}\")\n",
    "\n",
    "# Loss function/Optimizer: correcter or Greader\n",
    "# Calculate how far the guess is from the target\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Tool that adjust the weights and biases to minimize the loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop: practice\n",
    "print(\"\\nTraining...\")\n",
    "# epoch is the number of times the model will see the data\n",
    "for epoch in range(1000):\n",
    "    # A. Reset gradients (delete old notes)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # B. Forward pass: make a guess\n",
    "    output = model(inputs)\n",
    "\n",
    "    # C. Calculate loss\n",
    "    loss = criterion(output, targets) # Assign loss value\n",
    "\n",
    "    # D. Backward pass: calculate the gradient of the loss with respect to the weights and biases\n",
    "    loss.backward()\n",
    "\n",
    "    # E. Update the weights and biases\n",
    "    optimizer.step()\n",
    "\n",
    "    # F. Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} loss: {loss.item():2f}\")\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.tensor([[10.0]])\n",
    "prediction = model(test_input).item()\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Final Weight (should be ~2.0): {model.weight.item():.4f}\")\n",
    "print(f\"Final Bias (should be ~0.0): {model.bias.item():.4f}\")\n",
    "print(f\"Prediction for input 10.0: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f1a82",
   "metadata": {},
   "source": [
    "About Hidden Layers and Activation Function\n",
    "\n",
    "O2. Make an XOR Gate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb39c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XOR Solver...\n",
      "Epoch 0 Loss: 0.2560\n",
      "Epoch 1000 Loss: 0.1664\n",
      "Epoch 2000 Loss: 0.0274\n",
      "Epoch 3000 Loss: 0.0080\n",
      "Epoch 4000 Loss: 0.0042\n",
      "Epoch 5000 Loss: 0.0027\n",
      "Epoch 6000 Loss: 0.0020\n",
      "Epoch 7000 Loss: 0.0015\n",
      "Epoch 8000 Loss: 0.0013\n",
      "Epoch 9000 Loss: 0.0011\n",
      "\n",
      "--- RESULTS ---\n",
      "Inputs:\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "Target:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "AI Prediction:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. DATA: XOR Problem\n",
    "# Inputs: [A, B]\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "# Targets: Output (1 if one is true, 0 if both or neither is true)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 2. MODEL: Hidden Layer Network\n",
    "class SimpleNet(nn.Module):\n",
    "    # Multi-Layer Perceptron\n",
    "    def __init__(self):\n",
    "        # The 'super()' function is used to access methods of the parent class. Here, super(SimpleNet, self).__init__()\n",
    "        # specifically calls the __init__ constructor from nn.Module, making sure the SimpleNet\n",
    "        # instance is fully compatible with PyTorch's module system (like registering submodules,\n",
    "        # handling device moves, etc.). Skipping this call can cause unexpected errors down the road.\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Layer 1: Takes 2 inputs -> Transforms to 2 hidden features\n",
    "        # It maintains a 2 X 2 weight matrix and a bias vector of size 2\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        # Activation: ReLU (The magic non-linearity) allows network to learn complex pattern\n",
    "        self.relu = nn.ReLU()\n",
    "        # Layer 2: Takes 2 hidden features -> Decides 1 final output\n",
    "        self.output = nn.Linear(2, 1)\n",
    "        # Final Activation: Sigmoid (Squashes output between 0 and 1 for probability)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through hidden layer\n",
    "        x = self.hidden(x)\n",
    "        # Apply activation (turn on/off neurons) to remove linearity from neurons\n",
    "        x = self.relu(x)\n",
    "        # Pass to output layer\n",
    "        x = self.output(x)\n",
    "        # Squash to 0-1 range\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "# Stochastic Gradient Descent: logic that update weights {lr = 0.1(lerning rate)} step size how much we change weight after each iteration\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# Mean Squared Error. This is the Penalty: The goal of the training is to minimize this value.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"Training XOR Solver...\")\n",
    "# 3. TRAINING\n",
    "for epoch in range(10000):          # Needs more practice than simple math\n",
    "    optimizer.zero_grad()           # Clear previous calculation\n",
    "    outputs = model(X)              # Get Current guesses\n",
    "    loss = criterion(outputs, Y)    # Calculate Error\n",
    "    loss.backward()                 # calculates the gradient of the loss function with respect to each weight by propagating the error backward through the network\n",
    "    optimizer.step()                # It moves the weights in the opposite direction of the gradient to \"descend\" the error hill.\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 4. TEST\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "with torch.no_grad(): # Don't learn, just test\n",
    "    test_outputs = model(X)\n",
    "    predicted = (test_outputs > 0.5).float() # Convert probabilities to 0 or 1\n",
    "    print(f\"Inputs:\\n{X}\")\n",
    "    print(f\"Target:\\n{Y}\")\n",
    "    print(f\"AI Prediction:\\n{predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7439e43",
   "metadata": {},
   "source": [
    "Hands-On: The \"Game Loop\" for AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf02bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING GAME ---\n",
      "Initial State (Input): [ 0.03585678 -0.04110261 -0.03893887 -0.01395973]\n",
      "Step 0: Action 0 -> Reward 1.0 -> New State [ 0.03503472 -0.23564512 -0.03921806  0.2661877 ]\n",
      "Step 1: Action 0 -> Reward 1.0 -> New State [ 0.03032182 -0.43018603 -0.03389431  0.54624754]\n",
      "Step 2: Action 0 -> Reward 1.0 -> New State [ 0.0217181  -0.62481576 -0.02296936  0.82806146]\n",
      "Step 3: Action 0 -> Reward 1.0 -> New State [ 0.00922179 -0.81961626 -0.00640813  1.1134328 ]\n",
      "Step 4: Action 1 -> Reward 1.0 -> New State [-0.00717054 -0.62441075  0.01586053  0.8187465 ]\n",
      "Step 5: Action 0 -> Reward 1.0 -> New State [-0.01965875 -0.81974614  0.03223545  1.1163756 ]\n",
      "Step 6: Action 0 -> Reward 1.0 -> New State [-0.03605368 -1.0152761   0.05456297  1.4189936 ]\n",
      "Step 7: Action 1 -> Reward 1.0 -> New State [-0.0563592  -0.8208702   0.08294284  1.1438524 ]\n",
      "Step 8: Action 1 -> Reward 1.0 -> New State [-0.0727766  -0.626924    0.10581989  0.8782904 ]\n",
      "Step 9: Action 0 -> Reward 1.0 -> New State [-0.08531509 -0.82331246  0.12338569  1.2022783 ]\n",
      "Step 10: Action 1 -> Reward 1.0 -> New State [-0.10178133 -0.6299828   0.14743125  0.9506719 ]\n",
      "Step 11: Action 0 -> Reward 1.0 -> New State [-0.11438099 -0.8267485   0.1664447   1.2858064 ]\n",
      "Step 12: Action 0 -> Reward 1.0 -> New State [-0.13091595 -1.0235511   0.19216083  1.6256398 ]\n",
      "Step 13: Action 0 -> Reward 1.0 -> New State [-0.15138698 -1.2203441   0.22467363  1.9715441 ]\n",
      "!!! GAME OVER !!! Total Score: 14.0\n",
      "Step 14: Action 0 -> Reward 1.0 -> New State [-0.01538319 -0.17584474  0.02534922  0.33651757]\n",
      "Step 15: Action 0 -> Reward 1.0 -> New State [-0.01890009 -0.3713181   0.03207957  0.63708526]\n",
      "Step 16: Action 0 -> Reward 1.0 -> New State [-0.02632645 -0.56687236  0.04482128  0.93969554]\n",
      "Step 17: Action 1 -> Reward 1.0 -> New State [-0.0376639  -0.37238234  0.06361519  0.66142654]\n",
      "Step 18: Action 1 -> Reward 1.0 -> New State [-0.04511154 -0.17820054  0.07684372  0.38943326]\n",
      "Step 19: Action 0 -> Reward 1.0 -> New State [-0.04867556 -0.3743243   0.08463239  0.70532054]\n",
      "Step 20: Action 1 -> Reward 1.0 -> New State [-0.05616204 -0.18047063  0.0987388   0.44043306]\n",
      "Step 21: Action 1 -> Reward 1.0 -> New State [-0.05977145  0.01312545  0.10754746  0.18043543]\n",
      "Step 22: Action 0 -> Reward 1.0 -> New State [-0.05950894 -0.18335797  0.11115617  0.5050158 ]\n",
      "Step 23: Action 0 -> Reward 1.0 -> New State [-0.0631761  -0.37985644  0.12125649  0.8305571 ]\n",
      "Step 24: Action 0 -> Reward 1.0 -> New State [-0.07077323 -0.57640857  0.13786763  1.1587826 ]\n",
      "Step 25: Action 0 -> Reward 1.0 -> New State [-0.0823014  -0.773031    0.16104327  1.4913225 ]\n",
      "Step 26: Action 0 -> Reward 1.0 -> New State [-0.09776203 -0.96970487  0.19086973  1.8296584 ]\n",
      "Step 27: Action 0 -> Reward 1.0 -> New State [-0.11715612 -1.1663607   0.2274629   2.1750607 ]\n",
      "!!! GAME OVER !!! Total Score: 14.0\n",
      "Step 28: Action 1 -> Reward 1.0 -> New State [-0.02035479  0.19547991  0.00221052 -0.33537266]\n",
      "Step 29: Action 1 -> Reward 1.0 -> New State [-0.01644519  0.39057034 -0.00449693 -0.6273577 ]\n",
      "Step 30: Action 0 -> Reward 1.0 -> New State [-0.00863378  0.19551145 -0.01704409 -0.3360944 ]\n",
      "Step 31: Action 1 -> Reward 1.0 -> New State [-0.00472356  0.39087176 -0.02376598 -0.634103  ]\n",
      "Step 32: Action 0 -> Reward 1.0 -> New State [ 0.00309388  0.19608924 -0.03644804 -0.3489983 ]\n",
      "Step 33: Action 1 -> Reward 1.0 -> New State [ 0.00701566  0.3917101  -0.043428   -0.6529481 ]\n",
      "Step 34: Action 1 -> Reward 1.0 -> New State [ 0.01484987  0.5874091  -0.05648696 -0.95898354]\n",
      "Step 35: Action 1 -> Reward 1.0 -> New State [ 0.02659805  0.78324306 -0.07566664 -1.2688644 ]\n",
      "Step 36: Action 0 -> Reward 1.0 -> New State [ 0.04226291  0.58916456 -0.10104392 -1.0008043 ]\n",
      "Step 37: Action 0 -> Reward 1.0 -> New State [ 0.0540462   0.39552748 -0.12106001 -0.7414866 ]\n",
      "Step 38: Action 1 -> Reward 1.0 -> New State [ 0.06195675  0.5920941  -0.13588974 -1.0696834 ]\n",
      "Step 39: Action 1 -> Reward 1.0 -> New State [ 0.07379863  0.7887258  -0.15728341 -1.4017406 ]\n",
      "Step 40: Action 0 -> Reward 1.0 -> New State [ 0.08957314  0.59586847 -0.18531822 -1.1620762 ]\n",
      "Step 41: Action 1 -> Reward 1.0 -> New State [ 0.10149051  0.79285556 -0.20855975 -1.5066699 ]\n",
      "Step 42: Action 0 -> Reward 1.0 -> New State [ 0.11734763  0.60078263 -0.23869315 -1.2856768 ]\n",
      "!!! GAME OVER !!! Total Score: 15.0\n",
      "Step 43: Action 1 -> Reward 1.0 -> New State [ 0.04479854  0.19047531  0.00040782 -0.2438698 ]\n",
      "Step 44: Action 1 -> Reward 1.0 -> New State [ 0.04860804  0.38559145 -0.00446957 -0.53642404]\n",
      "Step 45: Action 0 -> Reward 1.0 -> New State [ 0.05631987  0.19053261 -0.01519805 -0.2451528 ]\n",
      "Step 46: Action 1 -> Reward 1.0 -> New State [ 0.06013053  0.3858683  -0.02010111 -0.54259056]\n",
      "Step 47: Action 1 -> Reward 1.0 -> New State [ 0.06784789  0.5812669  -0.03095292 -0.84153855]\n",
      "Step 48: Action 0 -> Reward 1.0 -> New State [ 0.07947323  0.38658085 -0.0477837  -0.55874807]\n",
      "Step 49: Action 0 -> Reward 1.0 -> New State [ 0.08720484  0.19216104 -0.05895865 -0.28149432]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 1. Setup the Game Environment\n",
    "# render_mode=\"human\" opens a window so you can watch\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset the game to start\n",
    "# 'state' is the Input for your Neural Network later\n",
    "state, info = env.reset()\n",
    "\n",
    "print(\"--- STARTING GAME ---\")\n",
    "print(f\"Initial State (Input): {state}\")\n",
    "# For CartPole, State is 4 numbers: [Cart Position, Cart Velocity, Pole Angle, Pole Velocity]\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(50):\n",
    "    # 2. Pick a Random Action\n",
    "    # In the future, your Neural Network will choose this!\n",
    "    # 0 = Push Left, 1 = Push Right\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # 3. Take the Action and see what happens\n",
    "    # new_state: The new numbers after moving\n",
    "    # reward: Points for keeping the pole upright (+1 per frame)\n",
    "    # terminated: Did we lose? (Pole fell)\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    print(f\"Step {step}: Action {action} -> Reward {reward} -> New State {new_state}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"!!! GAME OVER !!! Total Score: {total_reward}\")\n",
    "        # Reset to try again\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
